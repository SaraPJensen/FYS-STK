{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_new import *\n",
    "np.random.seed(1826)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data = cancer.data\n",
    "target = cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.2)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin a broad test to find the most optimal parameters w.r.t. accuracy, hence the cost-function in this case will be the cross-entropy cost function.\n",
    "The following is simply a collection of lists and arrays which contain parameter options which will be tested for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initialization = ['he', 'xavier', 'none', 'homemade'] # None => normally distributed\n",
    "activation_functions = ['sigmoid', 'relu', 'leaky_relu']\n",
    "learning_rates = [10**n for n in range(-6, 1)]\n",
    "batch_sizes = [2**n for n in range(5)]\n",
    "\n",
    "layer_sizes = np.arange(10, 40+1, 10)\n",
    "max_layers = 1\n",
    "\n",
    "layers = []\n",
    "for length in range(1, max_layers + 1):\n",
    "    l = []\n",
    "    for elem in product(layer_sizes, repeat = length):\n",
    "        l.append(list(elem))\n",
    "\n",
    "    layers.append(l)\n",
    "\n",
    "epochs = np.floor(np.linspace(1e3, 1e4, 6)).astype(int)\n",
    "\n",
    "lambdas = np.logspace(-4, 1, 6)\n",
    "cost_function = 'accuracy'\n",
    "method = 'classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of matrix: (84, 4, 3, 7, 5, 6, 6)\n",
      "Total elements: 1270080, same memory allocation as a square 1127.0-dim matrix (approx)\n"
     ]
    }
   ],
   "source": [
    "shape = [84] #  Number of hidden layer and node combinations tested\n",
    "shape += [len(weight_initialization), len(activation_functions), len(learning_rates),\\\n",
    "         len(batch_sizes), len(epochs), len(lambdas)]\n",
    "\n",
    "print(f\"Dimensionality of matrix: {tuple(shape)}\")\n",
    "prod = 1\n",
    "for elem in shape:\n",
    "    prod *= elem\n",
    "print(f\"Total elements: {prod}, same memory allocation as a square {np.ceil(np.sqrt(prod))}-dim matrix (approx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our monsterous for-loop may now commence.\n",
    "\n",
    "It was at this moment (21:26) Daniel realized that training a Neural Network 1270080 times would be quite unfeasable. The operation was stopped and after some minor changes and he went home at 22:02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/84, 0/4, 0/3, 0/7, 0/5, 0/6, 0/6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9916847b7cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                        weight_init_method=wi)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                 \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SGD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"no\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fys-Stk4155/FYS-STK/Project2/neural_new.py\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(self, method, plot)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fys-Stk4155/FYS-STK/Project2/neural_new.py\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#don't include the output layer or input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m#calculate all the errors before the weights and biases are updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fys-Stk4155/FYS-STK/Project2/neural_new.py\u001b[0m in \u001b[0;36mactivation_derivative\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivation_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fys-Stk4155/FYS-STK/Project2/neural_new.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "acc_scores = np.zeros(tuple(shape))\n",
    "\n",
    "for i in range(3):\n",
    "    for hl_i, hl in enumerate(layers[i]):\n",
    "        for wi_i, wi in enumerate(weight_initialization):\n",
    "            for af_i, af in enumerate(activation_functions):\n",
    "                for eta_i, eta in enumerate(learning_rates):\n",
    "                    for bs_i, bs in enumerate(batch_sizes):\n",
    "                        for ep_i, ep in enumerate(epochs):\n",
    "                            for l_i, lambd in enumerate(lambdas):\n",
    "                                \n",
    "                                if i == 1:\n",
    "                                    hl_i = 4 + hl_i\n",
    "                                elif i == 2:\n",
    "                                    hl_i = 20 + hl_i\n",
    "                                    \n",
    "                                print(f\"{hl_i}/{shape[0]}, {wi_i}/{shape[1]}, {af_i}/{shape[2]}, {eta_i}/{shape[3]}, {bs_i}/{shape[4]}, {ep_i}/{shape[5]}, {l_i}/{shape[6]}\")\n",
    "                                    \n",
    "                                network = NeuralNetwork(X_train_s, y_train, X_test_s, y_test,\\\n",
    "                                                       hl, ep, bs, eta, lambd, af,\\\n",
    "                                                       cost_func='accuracy', dataset='classification',\\\n",
    "                                                       weight_init_method=wi)\n",
    "                                \n",
    "                                network.model_training(\"SGD\", plot=\"no\")\n",
    "                                \n",
    "                                pred = network.prediction(X_test_s)\n",
    "\n",
    "                                print(pred)\n",
    "                                print(pred.round())\n",
    "                                print(pred.shape)\n",
    "                                print(y_test.reshape(-1, 1).shape)\n",
    "                                test_score = np.sum(pred.round() == y_test.reshape(1, -1)) / len(y_test)\n",
    "                                print(f\"Accuracy: {np.sum(pred.round() == y_test)}/{len(y_test)} = {test_score}\")\n",
    "                                \n",
    "                                acc_scores[hl_i, wi_i, af_i, eta_i, bs_i, ep_i, l_i] = test_score\n",
    "print(\"Matrix complete.\")\n",
    "'''       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we wish to find the optimal learning rate and hyperparameter $\\lambda$, so lets find a proper combination of the other parameters first. This way we ensure that the the optimal $\\eta$-$\\lambda$ combination will be a good approximation to the true optimal parameter-combination.\n",
    "\n",
    "Starting with the optimal layer and node combination, keeping every other parameter constant. \n",
    "\n",
    "We choose the sigmoid function as the activiation function and xavier as the weight initialization. $\\eta$ = 0.01, $\\lambda$ = 0, batch size = 1 and epochs = 1000.\n",
    "\n",
    "Starting with a fixed amount of nodes in each layer, 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with layerstructure: [30], commencing training\n",
      "Initialized with layerstructure: [30, 30], commencing training\n",
      "Initialized with layerstructure: [30, 30, 30], commencing training\n"
     ]
    }
   ],
   "source": [
    "hl = [[30], [30, 30], [30, 30, 30]]\n",
    "trained_models = []\n",
    "for layer in hl:\n",
    "    network = NeuralNetwork(X_train_s, y_train, X_test_s, y_test,\\\n",
    "                            layer, 1000, 1, 0.01, 0, 'sigmoid',\\\n",
    "                            cost_func='accuracy', dataset='classification',\\\n",
    "                            weight_init_method='xavier')\n",
    "    print(f\"Initialized with layerstructure: {layer}, commencing training\")\n",
    "    network.model_training(\"SGD\", plot='no')\n",
    "    trained_models.append(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 1)\n",
      "[112, 112, 112]\n"
     ]
    }
   ],
   "source": [
    "probabilities = []\n",
    "pred = []\n",
    "accs = []\n",
    "target = y_test.reshape(-1, 1)\n",
    "print(target.shape)\n",
    "for i in range(len(hl)):\n",
    "    #print(trained_models[i].hidden_nodes)\n",
    "    probabilities.append(trained_models[i].prediction(X_test_s))\n",
    "    pred.append(probabilities[i].round())\n",
    "    accs.append(np.sum(pred[i] == target))\n",
    "\n",
    "print(accs)\n",
    "#trained_models[0].prediction(X_test_s)\n",
    "#pred = probabilities.round()\n",
    "#print(pred)\n",
    "#print(sum(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as there is no difference when it comes to the number of layers, the hiddel layer structure [30, 30] will be used henceforth. That is two hidden layers each containing 30 nodes. \n",
    "\n",
    "Lets now test the other combination "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
